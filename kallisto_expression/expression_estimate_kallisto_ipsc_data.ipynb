{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from tools.fileOps import *\n",
    "from tools.procOps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constructed the transcript files\n",
    "m = {}\n",
    "# first, the CAT annotations\n",
    "for g in ['Clint_Chimp', 'Susie_Gorilla', 'Susie_Orangutan']:\n",
    "    gp = '../../consensus_gene_set/{}.gp'.format(g)\n",
    "    fa = '../../genome_files/{}.fa'.format(g)\n",
    "    with TemporaryFilePath() as tmp:\n",
    "        !genePredToBed {gp} {tmp}\n",
    "        !bedtools getfasta -fi {fa} -bed {tmp} -fo {g + '.CAT.transcripts.fa'} -name -split -s\n",
    "    m[(g.replace('_', '/'), 'CAT')] = g + '.CAT.transcripts.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CAT annotations for original primates, fix names\n",
    "og_dir = '/hive/groups/recon/projs/primates/original_primates/redo_annotation_indel/'\n",
    "for g, og in zip(*[['Chimp', 'Gorilla', 'Orangutan'], ['panTro4', 'gorGor4', 'ponAbe2']]):\n",
    "    gp = os.path.join(og_dir, 'consensus_gene_set', g + '.gp')\n",
    "    fa = os.path.join(og_dir, 'genome_files', g + '.fa')\n",
    "    with TemporaryFilePath() as tmp:\n",
    "        !genePredToBed {gp} {tmp}\n",
    "        !bedtools getfasta -fi {fa} -bed {tmp} -fo {og + '.CAT.transcripts.fa'} -name -split -s\n",
    "    m[(og, 'CAT')] = og + '.CAT.transcripts.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# human, which CAT already produces\n",
    "!cp ../../reference/gencode.v27.annotation.no_PAR.fa ./\n",
    "m[('Human', 'GENCODE V27')] = 'gencode.v27.annotation.no_PAR.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index file chimp_gorilla_fasta/gorilla_gorilla_softmasked_toplevel.fa.fixed.fai not found, generating...\n",
      "Feature (CABD030151935.1:1777-2008) beyond the length of CABD030151935.1 size (2000 bp).  Skipping.\n",
      "Feature (CABD030151492.1:1938-2009) beyond the length of CABD030151492.1 size (2000 bp).  Skipping.\n",
      "Feature (CABD030130636.1:1218-1325) beyond the length of CABD030130636.1 size (1322 bp).  Skipping.\n",
      "Feature (CABD030130064.1:746-923) beyond the length of CABD030130064.1 size (913 bp).  Skipping.\n",
      "Feature (CABD030151826.1:800-900) beyond the length of CABD030151826.1 size (894 bp).  Skipping.\n",
      "Feature (CABD030151588.1:765-872) beyond the length of CABD030151588.1 size (870 bp).  Skipping.\n",
      "Feature (CABD030161071.1:739-831) beyond the length of CABD030161071.1 size (818 bp).  Skipping.\n",
      "index file chimp_gorilla_fasta/pan_troglodytes_softmasked_toplevel.fa.fixed.fai not found, generating...\n",
      "Feature (KV421959.1:10660-10764) beyond the length of KV421959.1 size (10756 bp).  Skipping.\n",
      "Feature (AACZ04043439.1:1309-1416) beyond the length of AACZ04043439.1 size (1400 bp).  Skipping.\n",
      "Feature (AACZ04052657.1:1219-1326) beyond the length of AACZ04052657.1 size (1310 bp).  Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Ensembl V90 for chimp and gorilla\n",
    "!gtfToGenePred -genePredExt Pan_troglodytes.Pan_tro_3.0.91.gtf /dev/stdout | genePredToBed /dev/stdin Pan_troglodytes.Pan_tro_3.0.91.bed\n",
    "!gtfToGenePred -genePredExt Gorilla_gorilla.gorGor4.91.gtf /dev/stdout | genePredToBed /dev/stdin Gorilla_gorilla.gorGor4.91.bed\n",
    "!bedtools getfasta -fi chimp_gorilla_fasta/gorilla_gorilla_softmasked_toplevel.fa.fixed -bed Gorilla_gorilla.gorGor4.91.bed -name -split -s -fo gorGor4.Ensembl.transcripts.fa \n",
    "!bedtools getfasta -fi chimp_gorilla_fasta/pan_troglodytes_softmasked_toplevel.fa.fixed -bed Pan_troglodytes.Pan_tro_3.0.91.bed -name -split -s -fo panTro4.Ensembl.transcripts.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m[('gorGor4', 'Ensembl V90')] = 'gorGor4.Ensembl.transcripts.fa'\n",
    "m[('panTro4', 'Ensembl V90')] = 'panTro4.Ensembl.transcripts.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finally, we need to load the ensembl original annotations\n",
    "# for orangutan, things are easier\n",
    "with TemporaryFilePath() as tmp:\n",
    "    g = 'Orangutan'\n",
    "    og = 'ponAbe2'\n",
    "    fa = os.path.join(og_dir, 'genome_files', g + '.fa')\n",
    "    !gtfToGenePred -genePredExt Pongo_abelii.PPYG2.88.gtf /dev/stdout | genePredToBed /dev/stdin {tmp}\n",
    "    !bedtools getfasta -fi {fa} -bed {tmp} -fo {og + '.ensembl.transcripts.fa'} -name -split -s\n",
    "    m[('ponAbe2', 'Ensembl V90')] = og + '.ensembl.transcripts.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now, we can construct our kallisto indices\n",
    "\n",
    "!mkdir indices -p\n",
    "\n",
    "import os\n",
    "index_map = {}\n",
    "with open('cmds.txt', 'w') as outf:\n",
    "    for (g, a), x in m.iteritems():\n",
    "        o = '_'.join([g.replace('/', '_'), a.replace(' ', '_')])\n",
    "        cmd = 'kallisto index -i indices/{}.kallisto {}\\n'.format(o, x)\n",
    "        outf.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "[build] loading fasta file [build] loading fasta file Susie_Orangutan.CAT.transcripts.fapanTro4.Ensembl.transcripts.fa\n",
      "\n",
      "[build] k-mer length: [build] k-mer length: [build] loading fasta file [build] loading fasta file 31panTro4.CAT.transcripts.fa31gorGor4.CAT.transcripts.fa\n",
      "\n",
      "\n",
      "\n",
      "[build] k-mer length: [build] k-mer length: 3131\n",
      "\n",
      "\n",
      "[build] loading fasta file ponAbe2.ensembl.transcripts.fa\n",
      "[build] k-mer length: 31\n",
      "\n",
      "[build] loading fasta file ponAbe2.CAT.transcripts.fa\n",
      "[build] k-mer length: 31\n",
      "\n",
      "[build] loading fasta file gencode.v27.annotation.no_PAR.fa\n",
      "[build] k-mer length: 31\n",
      "\n",
      "[build] loading fasta file Clint_Chimp.CAT.transcripts.fa\n",
      "[build] k-mer length: 31\n",
      "\n",
      "[build] loading fasta file Susie_Gorilla.CAT.transcripts.fa\n",
      "[build] k-mer length: 31\n",
      "\n",
      "[build] loading fasta file gorGor4.Ensembl.transcripts.fa\n",
      "[build] k-mer length: 31\n",
      "[build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 55 target sequences\n",
      "[build] warning: replaced 49667 non-ACGUT characters in the input sequence\n",
      "        with pseudorandom nucleotides\n",
      "[build] counting k-mers ... [build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 6 target sequences\n",
      "[build] warning: replaced 24328 non-ACGUT characters in the input sequence\n",
      "        with pseudorandom nucleotides\n",
      "[build] counting k-mers ... [build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 6 target sequences\n",
      "[build] warning: replaced 6450 non-ACGUT characters in the input sequence\n",
      "        with pseudorandom nucleotides\n",
      "[build] counting k-mers ... [build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 1547 target sequences\n",
      "[build] warning: replaced 4 non-ACGUT characters in the input sequence\n",
      "        with pseudorandom nucleotides\n",
      "[build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 970 target sequences\n",
      "[build] warning: replaced 1700327 non-ACGUT characters in the input sequence\n",
      "        with pseudorandom nucleotides\n",
      "[build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 1192 target sequences\n",
      "[build] warning: replaced 675280 non-ACGUT characters in the input sequence\n",
      "        with pseudorandom nucleotides\n",
      "[build] counting k-mers ... [build] counting k-mers ... [build] counting k-mers ... [build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 1261 target sequences\n",
      "[build] counting k-mers ... [build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 1212 target sequences\n",
      "[build] warning: replaced 1797688 non-ACGUT characters in the input sequence\n",
      "        with pseudorandom nucleotides\n",
      "[build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 1241 target sequences\n",
      "[build] counting k-mers ... [build] counting k-mers ... [build] warning: clipped off poly-A tail (longer than 10)\n",
      "        from 1038 target sequences\n",
      "[build] counting k-mers ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ... done.\n",
      "[build] building target de Bruijn graph ...  done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 154195 contigs and contains 49217551 k-mers \n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 241537 contigs and contains 52944297 k-mers \n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 304574 contigs and contains 62105457 k-mers \n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 1083479 contigs and contains 115327291 k-mers \n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 1118440 contigs and contains 119719595 k-mers \n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 1143817 contigs and contains 120433128 k-mers \n",
      " done\n",
      "[build] target de Bruijn graph has 1170009 contigs and contains 120559417 k-mers \n",
      "\n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 1194880 contigs and contains 122868396 k-mers \n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 1219188 contigs and contains 123430526 k-mers \n",
      "\n",
      " done \n",
      "[build] creating equivalence classes ...  done\n",
      "[build] target de Bruijn graph has 1280279 contigs and contains 125700873 k-mers \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat cmds.txt | xargs -n 1 -P 10 -i sh -c \"{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map of fastq names to genome names\n",
    "fq_map = {'panTro4': 'chimp', 'ponAbe2': 'orang', 'Human': 'human', 'gorGor3': 'gorilla', 'gorGor4': 'gorilla',\n",
    "         'Susie/Gorilla': 'gorilla', 'Clint/Chimp': 'chimp', 'Susie/Orangutan': 'orang'}\n",
    "\n",
    "with open('kallisto_cmds.txt', 'w') as outf:\n",
    "    for (g, a), x in m.iteritems():\n",
    "        fwd = 'fastqs/' + fq_map[g] + '.fwd.fq'\n",
    "        rev = 'fastqs/' + fq_map[g] + '.rev.fq'\n",
    "        o = '_'.join([g.replace('/', '_'), a.replace(' ', '_')])\n",
    "        index = 'indices/' + o + '.kallisto'\n",
    "        cmd = 'kallisto quant -t 5 -o {} -i {} {} {}\\n'.format(o, index, fwd, rev)\n",
    "        outf.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] k-mer length: [index] number of targets: 31198,988\n",
      "\n",
      "[index] number of k-mers: 123,430,526\n",
      "[index] number of targets: 194,661\n",
      "[index] number of k-mers: 119,719,595\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 196,045\n",
      "[index] number of k-mers: 120,559,417\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 58,363\n",
      "[index] number of k-mers: 62,105,457\n",
      "[index] number of equivalence classes: 156,027\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/chimp.fwd.fq\n",
      "                             fastqs/chimp.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ...[index] number of equivalence classes: 770,166\n",
      "[index] number of equivalence classes: 800,009\n",
      "[index] number of equivalence classes: 751,332\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/chimp.fwd.fq\n",
      "                             fastqs/chimp.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ...[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/gorilla.fwd.fq\n",
      "                             fastqs/gorilla.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ...[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/chimp.fwd.fq\n",
      "                             fastqs/chimp.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 47,464,867 reads, 32,890,587 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 186.147\n",
      "[   em] quantifying the abundances ... done\n",
      "[quant] processed 47,270,439 reads, 33,539,774 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 174.003\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,147 rounds\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 193,727\n",
      "[index] number of k-mers: 115,327,291\n",
      " done\n",
      "[quant] processed 47,270,439 reads, 31,005,151 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 175.081\n",
      "[   em] quantifying the abundances ...[index] number of equivalence classes: 729,277\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/orang.fwd.fq\n",
      "                             fastqs/orang.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,412 rounds\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 29,410\n",
      "[index] number of k-mers: 49,217,551\n",
      "[index] number of equivalence classes: 73,169\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/orang.fwd.fq\n",
      "                             fastqs/orang.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,261 rounds\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 197,713\n",
      "[index] number of k-mers: 120,433,128\n",
      "[index] number of equivalence classes: 771,123\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/orang.fwd.fq\n",
      "                             fastqs/orang.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 38,095,759 reads, 33,409,679 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 179.262\n",
      "[   em] quantifying the abundances ... done\n",
      "[quant] processed 38,095,759 reads, 29,789,450 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 178.482\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,014 rounds\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 197,352\n",
      "[index] number of k-mers: 122,868,396\n",
      "[index] number of equivalence classes: 789,708\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/gorilla.fwd.fq\n",
      "                             fastqs/gorilla.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,446 rounds\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 199,808\n",
      "[index] number of k-mers: 125,700,873\n",
      " done\n",
      "[quant] processed 47,270,439 reads, 27,035,152 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 184.641\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,048 rounds\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 52,996\n",
      "[index] number of k-mers: 52,944,297\n",
      "[index] number of equivalence classes: 126,883\n",
      "[index] number of equivalence classes: 822,430\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/gorilla.fwd.fq\n",
      "                             fastqs/gorilla.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ...[quant] running in paired-end mode\n",
      "[quant] will process pair 1: fastqs/human.fwd.fq\n",
      "                             fastqs/human.rev.fq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 38,095,759 reads, 34,746,230 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 179.163\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,237 rounds\n",
      "\n",
      " done\n",
      "[quant] processed 47,464,867 reads, 28,827,885 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 186.964\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,100 rounds\n",
      "\n",
      " done\n",
      "[quant] processed 47,464,867 reads, 34,882,814 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 176.262\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,357 rounds\n",
      "\n",
      " done\n",
      "[quant] processed 68,769,111 reads, 49,179,580 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 174.377\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 1,332 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat kallisto_cmds.txt | xargs -n 1 -P 4 -i sh -c \"{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "dfs = {}\n",
    "for g, a in m:\n",
    "    o = '_'.join([g.replace('/', '_'), a.replace(' ', '_')])\n",
    "    df = pd.read_csv(os.path.join(o, 'abundance.tsv'), sep='\\t')\n",
    "    df['target_id'] = [x.split('(')[0] for x in df['target_id']]\n",
    "    dfs[(g, a)] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start combining\n",
    "\n",
    "from tools.bio import *\n",
    "from tools.transcripts import *\n",
    "from tools.sqlInterface import *\n",
    "from tools.gff3 import *\n",
    "from tools.misc import *\n",
    "from collections import *\n",
    "\n",
    "# load name maps\n",
    "gtfs = {('panTro4', 'Ensembl V91'): 'Pan_troglodytes.Pan_tro_3.0.91.gtf',\n",
    "     ('gorGor4', 'Ensembl V91'): 'Gorilla_gorilla.gorGor4.91.gtf',\n",
    "     ('ponAbe2', 'Ensembl V90'): 'Pongo_abelii.PPYG2.88.gtf',\n",
    "     ('panTro4', 'CAT'): '/hive/groups/recon/projs/primates/original_primates/redo_annotation_indel/consensus_gene_set/Chimp.gtf',\n",
    "     ('gorGor4', 'CAT'): '/hive/groups/recon/projs/primates/original_primates/redo_annotation_indel/consensus_gene_set/Gorilla.gtf',\n",
    "     ('ponAbe2', 'CAT'): '/hive/groups/recon/projs/primates/original_primates/redo_annotation_indel/consensus_gene_set/Orangutan.gtf',\n",
    "     ('Clint/Chimp', 'CAT'): '/hive/groups/recon/projs/primates/primates_indel_corrected_bionano_cut/consensus_gene_set/Clint_Chimp.gtf',\n",
    "     ('Susie/Gorilla', 'CAT'): '/hive/groups/recon/projs/primates/primates_indel_corrected_bionano_cut/consensus_gene_set/Susie_Gorilla.gtf',\n",
    "    ('Susie/Orangutan', 'CAT'): '/hive/groups/recon/projs/primates/primates_indel_corrected_bionano_cut/consensus_gene_set/Susie_Orangutan.gtf',\n",
    "    ('Human', 'GENCODE V27'): '/hive/groups/recon/projs/primates/primates_indel_corrected_bionano_cut/reference/gencode.v27.annotation.no_PAR.gtf'}\n",
    "\n",
    "def construct_ensembl_map(gtf):\n",
    "    lines = [x.split('\\t') for x in open(gtf) if not x.startswith('#')]\n",
    "    ensembl_map = []\n",
    "    for l in lines:\n",
    "        x = parse_gtf_attr_line(l[-1])\n",
    "        try:\n",
    "            ensembl_map.append([x['gene_id'], x['transcript_id']])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return pd.DataFrame(ensembl_map, columns=['gene_id', 'transcript_id'])\n",
    "\n",
    "\n",
    "name_dfs = defaultdict(dict)\n",
    "for (g, a), gtf in gtfs.iteritems():\n",
    "    if g == 'Human':\n",
    "        name_dfs[(g, a)] = load_annotation('/hive/groups/recon/projs/primates/susie_indel_corrected/databases/Human.db')\n",
    "        name_dfs[(g, a)] = name_dfs[(g, a)][['TranscriptId', 'GeneId']].drop_duplicates()\n",
    "        name_dfs[(g, a)].columns = ['transcript_id', 'gene_id']\n",
    "    elif 'Ensembl' in a:\n",
    "        name_dfs[(g, a)] = construct_ensembl_map(gtf)\n",
    "    else:\n",
    "        gp_info = gtf.replace('.gtf', '.gp_info')\n",
    "        df = pd.read_csv(gp_info, sep='\\t', header=0)[['gene_id', 'transcript_id']].drop_duplicates()\n",
    "        df.columns = ['gene_id', 'transcript_id']\n",
    "        name_dfs[(g, a)] = df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_dfs = {}\n",
    "for x, name_df in name_dfs.iteritems():\n",
    "    df = dfs[x]\n",
    "    merged = df.merge(name_df, left_on='target_id', right_on='transcript_id')\n",
    "    combined_dfs[x] = merged.drop(['target_id', 'length', 'eff_length', 'est_counts'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_by_gene = {}\n",
    "for x, df in combined_dfs.iteritems():\n",
    "    combined_by_gene[x] = df.drop(['transcript_id'], axis=1).groupby('gene_id').aggregate(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn this into a flat dataframe with the columns Assembly/Annotation, total, expressed\n",
    "import numpy as np\n",
    "order = ['Human (GENCODE V27)', 'panTro4 (Ensembl V91)', 'panTro4 (CAT)', 'Clint/Chimp (CAT)',\n",
    "         'gorGor4 (Ensembl V91)', 'gorGor4 (CAT)', 'Susie/Gorilla (CAT)',\n",
    "         'ponAbe2 (Ensembl V90)', 'ponAbe2 (CAT)', 'Susie/Orangutan (CAT)',\n",
    "         ]\n",
    "\n",
    "\n",
    "r = []\n",
    "for x, df in combined_by_gene.iteritems():\n",
    "    r.append(['{} ({})'.format(*x), len(df) - len(df[df.tpm > 0.1]), len(df[df.tpm > 0.1])])\n",
    "gene_df = pd.DataFrame(r, columns=['Assembly/Annotation', 'not_expressed', 'expressed'])\n",
    "gene_df['Assembly/Annotation'] = pd.Categorical(gene_df['Assembly/Annotation'], order, ordered=True)\n",
    "gene_df = gene_df.sort_values('Assembly/Annotation')\n",
    "\n",
    "gene_data = np.array(gene_df[['expressed', 'not_expressed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now by transcript\n",
    "# turn this into a flat dataframe with the columns Assembly/Annotation, total, expressed\n",
    "import numpy as np\n",
    "\n",
    "r = []\n",
    "for x, df in combined_dfs.iteritems():\n",
    "    r.append(['{} ({})'.format(*x), len(df) - len(df[df.tpm > 0.1]), len(df[df.tpm > 0.1])])\n",
    "tx_df = pd.DataFrame(r, columns=['Assembly/Annotation', 'not_expressed', 'expressed'])\n",
    "tx_df['Assembly/Annotation'] = pd.Categorical(tx_df['Assembly/Annotation'], order, ordered=True)\n",
    "tx_df = tx_df.sort_values('Assembly/Annotation')\n",
    "\n",
    "tx_data = np.array(tx_df[['expressed', 'not_expressed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "palette=[\"#CDBA74\", \n",
    "         '#89bfdd', '#549ece', '#2b7bbb',\n",
    "         '#fd8262', '#f54f39', '#d62221',\n",
    "         '#8fd18c', '#57b668', '#2c954c',\n",
    "         ]\n",
    "\n",
    "from cat.plots import *\n",
    "# plot it\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "majorLocator = MultipleLocator(10000)\n",
    "minorLocator = MultipleLocator(2500)\n",
    "mkfunc = lambda x, pos: '{}k'.format(int(float(x) / 1000))\n",
    "mkformatter = matplotlib.ticker.FuncFormatter(mkfunc)\n",
    "with open('expressed_ipsc_kallisto.pdf', 'w') as outf, PdfPages(outf) as pdf:\n",
    "    bar_width = 0.6\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8, 3), ncols=2)\n",
    "    ax1.xaxis.set_major_locator(MultipleLocator(10000))\n",
    "    ax1.xaxis.set_major_formatter(mkformatter)\n",
    "    ax1.xaxis.set_minor_locator(MultipleLocator(2500))\n",
    "    ax2.xaxis.set_major_locator(MultipleLocator(25000))\n",
    "    ax2.xaxis.set_major_formatter(mkformatter)\n",
    "    ax2.xaxis.set_minor_locator(MultipleLocator(5000))\n",
    "    for i, (exp, not_exp) in enumerate(gene_data):\n",
    "        b = ax1.barh(i, exp, left=0, height=bar_width, alpha=0.9, linewidth=0, color=palette[i])\n",
    "        b = ax1.barh(i, not_exp, left=exp, height=bar_width, hatch='//', alpha=0.45, ecolor='black',\n",
    "                   linewidth=0, color=palette[i])\n",
    "    for i, (exp, not_exp) in enumerate(tx_data):\n",
    "        b = ax2.barh(i, exp, left=0, height=bar_width, alpha=0.9, linewidth=0, color=palette[i])\n",
    "        b = ax2.barh(i, not_exp, left=exp, height=bar_width, hatch='//', alpha=0.45, ecolor='black',\n",
    "                   linewidth=0, color=palette[i])\n",
    "\n",
    "    sns.despine()\n",
    "    ax1.set_yticks(np.arange(len(tx_df)) + (bar_width / 2))\n",
    "    ax2.set_yticks(np.arange(len(tx_df)) + (bar_width / 2))\n",
    "    ax2.set_yticklabels('')\n",
    "    ax1.set_yticklabels(list(tx_df['Assembly/Annotation']))\n",
    "    ax2.set_xlabel('Number of transcripts')\n",
    "    ax1.set_xlabel('Number of genes')\n",
    "    fig.suptitle('Non-zero expression estimates of species-specific iPSC RNA-seq (Kallisto)')\n",
    "    multipage_close(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Assembly/Annotation  not_expressed  expressed\n",
      "8    Human (GENCODE V27)          29945      27068\n",
      "1  panTro4 (Ensembl V91)          11595      20353\n",
      "2          panTro4 (CAT)          28173      28608\n",
      "3      Clint/Chimp (CAT)          28560      28441\n",
      "9  gorGor4 (Ensembl V91)          10420      19176\n",
      "0          gorGor4 (CAT)          27996      28385\n",
      "7    Susie/Gorilla (CAT)          28988      27988\n",
      "5  ponAbe2 (Ensembl V90)          13069      15337\n",
      "4          ponAbe2 (CAT)          35387      20389\n",
      "6  Susie/Orangutan (CAT)          36065      20349\n"
     ]
    }
   ],
   "source": [
    "print gene_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
